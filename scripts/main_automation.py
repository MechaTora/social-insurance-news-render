#!/usr/bin/env python3
"""
ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†ã‚·ã‚¹ãƒ†ãƒ 
GitHubãƒªãƒã‚¸ãƒˆãƒªç”¨ - main_automation.py

ä¸»ãªæ”¹å–„ç‚¹:
1. Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®URLã‚’æ­£ç¢ºã«å–å¾—
2. URLã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
3. æ”¹å–„ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
4. ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime
from urllib.parse import urljoin, urlparse
import re
import hashlib

class SocialInsuranceNewsScraper:
    def __init__(self):
        self.session = requests.Session()
        
        # User-Agentã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        self.categories = {
            'å¥åº·ä¿é™º': ['å¥åº·ä¿é™º', 'å”ä¼šã‘ã‚“ã½', 'å¥åº·ä¿é™ºçµ„åˆ', 'å›½æ°‘å¥åº·ä¿é™º'],
            'åšç”Ÿå¹´é‡‘': ['åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'ã­ã‚“ãã‚“', 'å¹´é‡‘'],
            'é›‡ç”¨ä¿é™º': ['é›‡ç”¨ä¿é™º', 'å¤±æ¥­ä¿é™º', 'é›‡ç”¨ç¶­æŒ', 'ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯'],
            'åŠ´ç½ä¿é™º': ['åŠ´ç½ä¿é™º', 'åŠ´åƒç½å®³', 'åŠ´ç½'],
            'ä»‹è­·ä¿é™º': ['ä»‹è­·ä¿é™º', 'è¦ä»‹è­·', 'ã‚±ã‚¢ãƒãƒ', 'ä»‹è­·èªå®š'],
            'ç¤¾ä¼šä¿é™ºå…¨èˆ¬': ['ç¤¾ä¼šä¿é™º', 'ç¤¾ä¼šä¿éšœ', 'åšç”ŸåŠ´åƒçœ']
        }
        
    def is_valid_url(self, url):
        """URLã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc]) and result.scheme in ['http', 'https']
        except:
            return False
    
    def safe_request(self, url, timeout=15, retries=3):
        """å®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                return response
            except Exception as e:
                print(f"âš ï¸ Request failed (attempt {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                else:
                    return None
        return None
    
    def scrape_yahoo_news(self, search_term, max_articles=5):
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        articles = []
        search_url = f"https://news.yahoo.co.jp/search?p={search_term}&ei=UTF-8"
        
        print(f"ğŸ” Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢: {search_term}")
        
        response = self.safe_request(search_url)
        if not response:
            print(f"âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å¤±æ•—: {search_term}")
            return articles
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
        article_links = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: è¨˜äº‹URLç›´æ¥
        links1 = soup.select('a[href*="/articles/"]')
        article_links.extend(links1)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰å†…ã®ãƒªãƒ³ã‚¯
        links2 = soup.select('[class*="news"] a[href*="/articles/"]')
        article_links.extend(links2)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸€èˆ¬çš„ãªãƒªãƒ³ã‚¯ï¼ˆ/articles/ã‚’å«ã‚€ï¼‰
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            if '/articles/' in href and 'news.yahoo.co.jp' in href:
                article_links.append(link)
        
        print(f"ğŸ“° ç™ºè¦‹ã•ã‚ŒãŸãƒªãƒ³ã‚¯æ•°: {len(article_links)}")
        
        processed_urls = set()
        
        for link in article_links:
            if len(articles) >= max_articles:
                break
                
            href = link.get('href', '')
            if not href or href in processed_urls:
                continue
                
            # å®Œå…¨URLã«å¤‰æ›
            if href.startswith('/'):
                full_url = f"https://news.yahoo.co.jp{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
                
            if not self.is_valid_url(full_url) or full_url in processed_urls:
                continue
                
            processed_urls.add(full_url)
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = link.get_text(strip=True)
            if not title or len(title) < 5:
                continue
                
            # ç¤¾ä¼šä¿é™ºé–¢é€£ã®è¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯
            if not self.is_social_insurance_related(title):
                continue
                
            # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
            category = self.categorize_article(title)
            
            # é‡è¦åº¦åˆ¤å®š
            importance = self.assess_importance(title)
            
            article = {
                'id': self.generate_id(title, full_url),
                'title': title,
                'url': full_url,
                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                'category': category,
                'summary': f"ã€{category}ã€‘ {title[:50]}{'...' if len(title) > 50 else ''}",
                'importance': importance,
                'published_date': None,  # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã¯å–å¾—ãŒå›°é›£
                'scraped_at': datetime.now().isoformat(),
                'keywords': self.extract_keywords(title),
                'related_categories': [],
                'content_length': len(title),
                'confidence_score': self.calculate_confidence(title, category)
            }
            
            articles.append(article)
            print(f"âœ… Yahoo!è¨˜äº‹è¿½åŠ : {title[:50]}...")
            
        return articles
    
    def scrape_mhlw_news(self, max_articles=10):
        """åšç”ŸåŠ´åƒçœã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        articles = []
        
        # è¤‡æ•°ã®MHLWãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
        mhlw_urls = [
            'https://www.mhlw.go.jp/stf/houdou/houdou_list.html',
            'https://www.mhlw.go.jp/stf/houdou/',
        ]
        
        for base_url in mhlw_urls:
            print(f"ğŸ›ï¸ åšç”ŸåŠ´åƒçœã‚µã‚¤ãƒˆå–å¾—: {base_url}")
            
            response = self.safe_request(base_url)
            if not response:
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # MHLWã‚µã‚¤ãƒˆã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            article_links = []
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å ±é“ç™ºè¡¨ãƒªãƒ³ã‚¯
            links1 = soup.select('a[href*="/stf/houdou/"]')
            article_links.extend(links1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ–°ç€æƒ…å ±ãƒªãƒ³ã‚¯
            links2 = soup.select('a[href*="/stf/newpage_"]')
            article_links.extend(links2)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸€èˆ¬çš„ãªãƒªãƒ³ã‚¯ã§ç¤¾ä¼šä¿é™ºé–¢é€£
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                title = link.get_text(strip=True)
                if self.is_social_insurance_related(title):
                    article_links.append(link)
            
            processed_urls = set()
            
            for link in article_links:
                if len(articles) >= max_articles:
                    break
                    
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or href in processed_urls:
                    continue
                    
                # å®Œå…¨URLã«å¤‰æ›
                if href.startswith('/'):
                    full_url = f"https://www.mhlw.go.jp{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                    
                if not self.is_valid_url(full_url):
                    continue
                    
                processed_urls.add(href)
                
                # ç¤¾ä¼šä¿é™ºé–¢é€£ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self.is_social_insurance_related(title):
                    continue
                    
                # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
                category = self.categorize_article(title)
                
                # é‡è¦åº¦åˆ¤å®š
                importance = self.assess_importance(title)
                
                # å…¬é–‹æ—¥ã‚’æŠ½å‡ºï¼ˆè©¦è¡Œï¼‰
                published_date = self.extract_mhlw_date(soup, link)
                
                article = {
                    'id': self.generate_id(title, full_url),
                    'title': title,
                    'url': full_url,
                    'source': 'åšç”ŸåŠ´åƒçœ',
                    'category': category,
                    'summary': f"ã€{category}ã€‘ {title[:50]}{'...' if len(title) > 50 else ''}",
                    'importance': importance,
                    'published_date': published_date,
                    'scraped_at': datetime.now().isoformat(),
                    'keywords': self.extract_keywords(title),
                    'related_categories': [],
                    'content_length': len(title),
                    'confidence_score': self.calculate_confidence(title, category)
                }
                
                articles.append(article)
                print(f"âœ… MHLWè¨˜äº‹è¿½åŠ : {title[:50]}...")
                
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            time.sleep(random.uniform(2, 4))
            
        return articles
    
    def is_social_insurance_related(self, title):
        """ç¤¾ä¼šä¿é™ºé–¢é€£ã®è¨˜äº‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        social_insurance_keywords = [
            'å¹´é‡‘', 'ä¿é™º', 'ç¤¾ä¼šä¿é™º', 'å¥åº·', 'é›‡ç”¨', 'åŠ´ç½', 'ä»‹è­·',
            'åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'åŠ´ç½ä¿é™º', 'ä»‹è­·ä¿é™º',
            'ç¤¾ä¼šä¿éšœ', 'åšç”ŸåŠ´åƒçœ', 'ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯', 'çµ¦ä»˜'
        ]
        
        return any(keyword in title for keyword in social_insurance_keywords)
    
    def categorize_article(self, title):
        """è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword in title:
                    return category
        return 'ãã®ä»–'
    
    def assess_importance(self, title):
        """è¨˜äº‹ã®é‡è¦åº¦ã‚’è©•ä¾¡"""
        high_priority = ['æ”¹æ­£', 'æ–°åˆ¶åº¦', 'å»ƒæ­¢', 'å¤‰æ›´', 'é‡è¦', 'ç·Šæ€¥', 'é€Ÿå ±', 'æ–½è¡Œ']
        medium_priority = ['å®Ÿæ–½', 'é–‹å§‹', 'å»¶é•·', 'æ‹¡å¤§', 'è¦‹ç›´ã—', 'ç™ºè¡¨']
        
        for keyword in high_priority:
            if keyword in title:
                return 'é«˜'
                
        for keyword in medium_priority:
            if keyword in title:
                return 'ä¸­'
                
        return 'ä½'
    
    def extract_keywords(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        money_pattern = r'(\d+(?:,\d{3})*(?:\.\d+)?(?:ä¸‡|åƒ|å„„|å…†)?å††)'
        keywords.extend(re.findall(money_pattern, title))
        
        # å¹´æœˆãƒ‘ã‚¿ãƒ¼ãƒ³
        date_pattern = r'(\d{4}å¹´\d{1,2}æœˆ|\d{1,2}æœˆ\d{1,2}æ—¥|ä»¤å’Œ\d+å¹´)'
        keywords.extend(re.findall(date_pattern, title))
        
        # æ©Ÿé–¢å
        institutions = ['åšç”ŸåŠ´åƒçœ', 'æ—¥æœ¬å¹´é‡‘æ©Ÿæ§‹', 'å”ä¼šã‘ã‚“ã½', 'å¥åº·ä¿é™ºçµ„åˆ']
        for inst in institutions:
            if inst in title:
                keywords.append(inst)
        
        return keywords[:5]  # æœ€å¤§5å€‹
    
    def extract_mhlw_date(self, soup, link):
        """åšç”ŸåŠ´åƒçœè¨˜äº‹ã®å…¬é–‹æ—¥ã‚’æŠ½å‡º"""
        try:
            # è¦ªè¦ç´ ã‹ã‚‰æ—¥ä»˜ã‚’æ¢ã™
            parent = link.find_parent()
            if parent:
                date_text = parent.get_text()
                
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                date_patterns = [
                    r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                    r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                    r'(\d{4})-(\d{1,2})-(\d{1,2})'
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, date_text)
                    if match:
                        return match.group(0)
            return None
        except:
            return None
    
    def calculate_confidence(self, title, category):
        """è¨˜äº‹ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•
        if 10 <= len(title) <= 100:
            score += 0.2
            
        # ã‚«ãƒ†ã‚´ãƒªã®é©åˆåº¦
        if category != 'ãã®ä»–':
            score += 0.2
            
        # å°‚é–€ç”¨èªã®æœ‰ç„¡
        technical_terms = ['ä¿é™ºæ–™', 'çµ¦ä»˜', 'é©ç”¨', 'èªå®š', 'ç”³è«‹', 'åˆ¶åº¦']
        if any(term in title for term in technical_terms):
            score += 0.1
            
        return min(score, 1.0)
    
    def generate_id(self, title, url):
        """è¨˜äº‹IDã‚’ç”Ÿæˆ"""
        combined = f"{title}{url}"
        return hashlib.md5(combined.encode()).hexdigest()[:12]
    
    def run_collection(self):
        """ãƒ¡ã‚¤ãƒ³ã®åé›†å‡¦ç†"""
        print("ğŸš€ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        
        all_articles = []
        
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰åé›†
        yahoo_terms = ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'ä»‹è­·ä¿é™º']
        for term in yahoo_terms:
            articles = self.scrape_yahoo_news(term, max_articles=3)
            all_articles.extend(articles)
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            time.sleep(random.uniform(3, 6))
        
        # åšç”ŸåŠ´åƒçœã‹ã‚‰åé›†
        mhlw_articles = self.scrape_mhlw_news(max_articles=10)
        all_articles.extend(mhlw_articles)
        
        # é‡è¤‡é™¤å»
        unique_articles = {}
        for article in all_articles:
            article_id = article['id']
            if article_id not in unique_articles:
                unique_articles[article_id] = article
        
        final_articles = list(unique_articles.values())
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        categorized = {}
        for article in final_articles:
            category = article['category']
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(article)
        
        # çµ±è¨ˆè¨ˆç®—
        high_importance = sum(1 for article in final_articles if article['importance'] == 'é«˜')
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        result = {
            'last_updated': datetime.now().isoformat(),
            'total_count': len(final_articles),
            'categories': categorized
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        with open('data/processed_news.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        daily_report = {
            'date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'summary': {
                'total_news': len(final_articles),
                'high_importance': high_importance,
                'categories': list(categorized.keys()),
                'top_keywords': self.get_top_keywords(final_articles)
            },
            'highlights': [
                article for article in final_articles 
                if article['importance'] == 'é«˜'
            ][:5]
        }
        
        with open('data/daily_report.json', 'w', encoding='utf-8') as f:
            json.dump(daily_report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åé›†å®Œäº†: {len(final_articles)}ä»¶ã®è¨˜äº‹")
        print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(categorized)}")
        print(f"ğŸ”¥ é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹: {high_importance}ä»¶")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        for category, articles in categorized.items():
            print(f"   {category}: {len(articles)}ä»¶")
        
        return result
    
    def get_top_keywords(self, articles):
        """è¨˜äº‹ã‹ã‚‰ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keyword_count = {}
        for article in articles:
            for keyword in article.get('keywords', []):
                keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½10å€‹ã‚’è¿”ã™
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, count in sorted_keywords[:10]]

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = SocialInsuranceNewsScraper()
    scraper.run_collection()

if __name__ == "__main__":
    main()