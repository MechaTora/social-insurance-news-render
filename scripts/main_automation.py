#!/usr/bin/env python3
"""
Renderç‰ˆ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¯æœ4æ™‚å®Ÿè¡Œ - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°â†’AIè¦ç´„â†’ãƒ‡ãƒ¼ã‚¿æ›´æ–°
"""

import sys
import os
import json
import traceback
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import time
import random

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / 'data'
DATA_DIR.mkdir(exist_ok=True)

class RenderNewsAutomation:
    """Renderç”¨ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.processed_file = DATA_DIR / 'processed_news.json'
        self.report_file = DATA_DIR / 'daily_report.json'
    
    def scrape_mhlw_news(self):
        """åšç”ŸåŠ´åƒçœãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"""
        news_list = []
        
        try:
            print("ğŸ›ï¸ åšç”ŸåŠ´åƒçœãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
            
            # åšåŠ´çœãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹
            mhlw_urls = [
                'https://www.mhlw.go.jp/stf/houdou/houdou_list.html',
                'https://www.mhlw.go.jp/stf/newpage_index.html'
            ]
            
            for url in mhlw_urls:
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’æŠ½å‡º
                    links = soup.find_all('a', href=True)
                    
                    for link in links[:10]:  # ä¸Šä½10ä»¶
                        href = link.get('href', '')
                        title = link.get_text(strip=True)
                        
                        if (len(title) > 10 and 
                            ('å¹´é‡‘' in title or 'ä¿é™º' in title or 'ç¤¾ä¼šä¿é™º' in title)):
                            
                            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                            if href.startswith('/'):
                                full_url = 'https://www.mhlw.go.jp' + href
                            else:
                                full_url = href
                            
                            news_item = {
                                'title': title,
                                'url': full_url,
                                'source': 'åšç”ŸåŠ´åƒçœ',
                                'category': self.categorize_news(title),
                                'importance': self.assess_importance(title),
                                'summary': f"ã€{self.categorize_news(title)}ã€‘ {title[:50]}...",
                                'keywords': self.extract_keywords(title),
                                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                'scraped_at': datetime.now().isoformat()
                            }
                            
                            news_list.append(news_item)
                    
                    time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                    
                except Exception as e:
                    print(f"åšåŠ´çœURLå–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    continue
            
            print(f"âœ… åšåŠ´çœãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_list)}ä»¶å–å¾—")
            return news_list
            
        except Exception as e:
            print(f"âŒ åšåŠ´çœãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_yahoo_news(self):
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ç¤¾ä¼šä¿é™ºé–¢é€£å–å¾—"""
        news_list = []
        
        try:
            print("ğŸ“º Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
            
            # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼ˆç¤¾ä¼šä¿é™ºé–¢é€£ï¼‰
            search_terms = ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'å¹´é‡‘æ”¹æ­£']
            
            for term in search_terms:
                try:
                    search_url = f"https://news.yahoo.co.jp/search?p={term}&ei=UTF-8"
                    response = self.session.get(search_url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º
                    articles = soup.find_all('div', class_='newsFeed_item')
                    
                    for article in articles[:5]:  # å„æ¤œç´¢èªã§5ä»¶
                        try:
                            link_elem = article.find('a')
                            if not link_elem:
                                continue
                            
                            title = link_elem.get_text(strip=True)
                            url = link_elem.get('href', '')
                            
                            if url.startswith('/'):
                                url = 'https://news.yahoo.co.jp' + url
                            
                            news_item = {
                                'title': title,
                                'url': url,
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'category': self.categorize_news(title),
                                'importance': self.assess_importance(title),
                                'summary': f"ã€{self.categorize_news(title)}ã€‘ {title[:60]}...",
                                'keywords': self.extract_keywords(title),
                                'published_date': 'None',
                                'scraped_at': datetime.now().isoformat()
                            }
                            
                            news_list.append(news_item)
                            
                        except Exception as e:
                            print(f"Yahooè¨˜äº‹è§£æã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    print(f"Yahooæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {term}: {e}")
                    continue
            
            print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_list)}ä»¶å–å¾—")
            return news_list
            
        except Exception as e:
            print(f"âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def categorize_news(self, title):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        title_lower = title.lower()
        
        if 'å¥åº·ä¿é™º' in title or 'åŒ»ç™‚ä¿é™º' in title:
            return 'å¥åº·ä¿é™º'
        elif 'åšç”Ÿå¹´é‡‘' in title or 'å¹´é‡‘' in title:
            return 'åšç”Ÿå¹´é‡‘'
        elif 'é›‡ç”¨ä¿é™º' in title or 'å¤±æ¥­' in title:
            return 'é›‡ç”¨ä¿é™º'
        elif 'åŠ´ç½' in title or 'åŠ´åƒç½å®³' in title:
            return 'åŠ´ç½ä¿é™º'
        elif 'ä»‹è­·ä¿é™º' in title or 'ä»‹è­·' in title:
            return 'ä»‹è­·ä¿é™º'
        elif 'ç¤¾ä¼šä¿é™º' in title:
            return 'ç¤¾ä¼šä¿é™ºå…¨èˆ¬'
        else:
            return 'ãã®ä»–'
    
    def assess_importance(self, title):
        """é‡è¦åº¦åˆ¤å®š"""
        high_keywords = ['æ”¹æ­£', 'å¤‰æ›´', 'å¼•ãä¸Šã’', 'å¼•ãä¸‹ã’', 'æ–°åˆ¶åº¦', 'å»ƒæ­¢', 'ç·Šæ€¥']
        medium_keywords = ['è¦‹ç›´ã—', 'æ¤œè¨', 'äºˆå®š', 'æ¡ˆ', 'æ–¹é‡']
        
        title_lower = title.lower()
        
        if any(keyword in title_lower for keyword in high_keywords):
            return 'é«˜'
        elif any(keyword in title_lower for keyword in medium_keywords):
            return 'ä¸­'
        else:
            return 'ä½'
    
    def extract_keywords(self, text):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        import re
        
        # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        money_patterns = re.findall(r'\d+(?:,\d+)*(?:ä¸‡å††|å††|å„„å††)', text)
        
        # å¹´åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³  
        year_patterns = re.findall(r'(?:ä»¤å’Œ|å¹³æˆ)?\d+å¹´(?:\d+æœˆ)?', text)
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_words = []
        keywords = ['åšç”ŸåŠ´åƒçœ', 'å¹´é‡‘æ©Ÿæ§‹', 'ç¤¾ä¼šä¿é™º', 'æ”¹æ­£æ¡ˆ', 'æ–°åˆ¶åº¦']
        
        for keyword in keywords:
            if keyword in text:
                important_words.append(keyword)
        
        return money_patterns + year_patterns + important_words
    
    def generate_daily_report(self, news_data):
        """æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            total_news = len(news_data)
            high_importance = len([n for n in news_data if n.get('importance') == 'é«˜'])
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
            categories = {}
            for news in news_data:
                cat = news.get('category', 'ãã®ä»–')
                categories[cat] = categories.get(cat, 0) + 1
            
            # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            all_keywords = []
            for news in news_data:
                all_keywords.extend(news.get('keywords', []))
            
            keyword_count = {}
            for keyword in all_keywords:
                keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
            
            top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:10]
            
            report = {
                'generated_at': datetime.now().isoformat(),
                'summary': {
                    'total_news': total_news,
                    'high_importance': high_importance,
                    'categories': list(categories.keys()),
                    'top_keywords': [k[0] for k in top_keywords]
                },
                'categories': categories,
                'keywords': dict(top_keywords)
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            with open(self.report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {total_news}ä»¶")
            return report
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_processed_data(self, news_data):
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            # é‡è¤‡é™¤å»
            unique_news = []
            seen_urls = set()
            
            for news in news_data:
                url = news.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_news.append(news)
            
            # é‡è¦åº¦é †ãƒ»æ–°ã—ã„é †ã§ã‚½ãƒ¼ãƒˆ
            importance_order = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1}
            unique_news.sort(
                key=lambda x: (
                    importance_order.get(x.get('importance', 'ä½'), 1),
                    x.get('scraped_at', '')
                ),
                reverse=True
            )
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
            processed_data = {
                'last_updated': datetime.now().isoformat(),
                'total_count': len(unique_news),
                'news': unique_news,
                'categories': {}
            }
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
            for news in unique_news:
                category = news.get('category', 'ãã®ä»–')
                if category not in processed_data['categories']:
                    processed_data['categories'][category] = []
                processed_data['categories'][category].append(news)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            with open(self.processed_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {len(unique_news)}ä»¶")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_automation(self):
        """è‡ªå‹•åŒ–å®Ÿè¡Œ"""
        try:
            print("ğŸš€ Renderç‰ˆç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–é–‹å§‹")
            print(f"â° é–‹å§‹æ™‚åˆ»: {self.start_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
            print("-" * 60)
            
            # Step 1: ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            all_news = []
            
            # åšåŠ´çœãƒ‹ãƒ¥ãƒ¼ã‚¹
            mhlw_news = self.scrape_mhlw_news()
            all_news.extend(mhlw_news)
            
            # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹
            yahoo_news = self.scrape_yahoo_news()
            all_news.extend(yahoo_news)
            
            if not all_news:
                print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å¤±æ•—")
                return False
            
            print(f"ğŸ“¡ ç·åé›†ä»¶æ•°: {len(all_news)}ä»¶")
            
            # Step 2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ä¿å­˜
            if not self.save_processed_data(all_news):
                print("âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—")
                return False
            
            # Step 3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self.generate_daily_report(all_news)
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
            end_time = datetime.now()
            duration = end_time - self.start_time
            
            print("-" * 60)
            print("ğŸ‰ è‡ªå‹•åŒ–å®Œäº†!")
            print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration.total_seconds():.1f}ç§’")
            print(f"ğŸ“Š å‡¦ç†ä»¶æ•°: {len(all_news)}ä»¶")
            print(f"ğŸ• å®Œäº†æ™‚åˆ»: {end_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è‡ªå‹•åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print("è©³ç´°ã‚¨ãƒ©ãƒ¼:")
            traceback.print_exc()
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    automation = RenderNewsAutomation()
    success = automation.run_automation()
    
    if success:
        print("\nâœ… ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–æˆåŠŸ!")
        sys.exit(0)
    else:
        print("\nâŒ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åŒ–å¤±æ•—!")
        sys.exit(1)

if __name__ == "__main__":
    main()