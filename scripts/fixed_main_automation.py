#!/usr/bin/env python3
"""
ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆVer 2.0 - GitHubå½¢å¼å¯¾å¿œï¼‰
app.pyã®æœŸå¾…ã™ã‚‹å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime
from urllib.parse import urljoin, urlparse
import re
import hashlib

class SocialInsuranceNewsScraperFixed:
    def __init__(self):
        self.session = requests.Session()
        
        # ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache'
        })
        
        self.categories = {
            'å¥åº·ä¿é™º': ['å¥åº·ä¿é™º', 'å”ä¼šã‘ã‚“ã½', 'å¥åº·ä¿é™ºçµ„åˆ', 'å›½æ°‘å¥åº·ä¿é™º'],
            'åšç”Ÿå¹´é‡‘': ['åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'ã­ã‚“ãã‚“', 'å¹´é‡‘'],
            'é›‡ç”¨ä¿é™º': ['é›‡ç”¨ä¿é™º', 'å¤±æ¥­ä¿é™º', 'é›‡ç”¨ç¶­æŒ', 'ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯'],
            'åŠ´ç½ä¿é™º': ['åŠ´ç½ä¿é™º', 'åŠ´åƒç½å®³', 'åŠ´ç½'],
            'ä»‹è­·ä¿é™º': ['ä»‹è­·ä¿é™º', 'è¦ä»‹è­·', 'ã‚±ã‚¢ãƒãƒ', 'ä»‹è­·èªå®š'],
            'ç¤¾ä¼šä¿é™ºå…¨èˆ¬': ['ç¤¾ä¼šä¿é™º', 'ç¤¾ä¼šä¿éšœ', 'åšç”ŸåŠ´åƒçœ']
        }
        
    def safe_request(self, url, timeout=15, retries=2):
        """å®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                return response
            except Exception as e:
                print(f"âš ï¸ Request failed (attempt {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 4))
                else:
                    return None
        return None
    
    def scrape_yahoo_news_direct(self, search_term, max_articles=3):
        """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥çš„ã«è¨˜äº‹ã‚’å–å¾—"""
        articles = []
        
        search_url = f"https://news.yahoo.co.jp/search?p={search_term}&ei=UTF-8"
        print(f"ğŸ” Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢: {search_term}")
        
        response = self.safe_request(search_url)
        if not response:
            return articles
            
        # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã‚’æŠ½å‡º
        page_text = response.text
        
        # æ­£è¦è¡¨ç¾ã§Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¨˜äº‹URLã‚’ç›´æ¥æŠ½å‡º
        article_url_pattern = r'https://news\.yahoo\.co\.jp/articles/[a-zA-Z0-9]+'
        found_urls = re.findall(article_url_pattern, page_text)
        
        # é‡è¤‡é™¤å»
        unique_urls = list(set(found_urls))
        print(f"ğŸ“° ç™ºè¦‹ã•ã‚ŒãŸè¨˜äº‹URLæ•°: {len(unique_urls)}")
        
        for article_url in unique_urls[:max_articles * 2]:  # å¤šã‚ã«å–å¾—
            if len(articles) >= max_articles:
                break
                
            # å„è¨˜äº‹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            article_data = self.get_article_details(article_url)
            if article_data and self.is_social_insurance_related(article_data['title']):
                articles.append(article_data)
                print(f"âœ… Yahoo!è¨˜äº‹è¿½åŠ : {article_data['title'][:40]}...")
                
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(random.uniform(1, 2))
                
        return articles
    
    def get_article_details(self, article_url):
        """è¨˜äº‹URLã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            response = self.safe_request(article_url)
            if not response:
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¤‡æ•°ã®æ–¹æ³•ã§å–å¾—
            title = ""
            
            # æ–¹æ³•1: titleã‚¿ã‚°
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                title = title.replace(' - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹', '').replace('ï½œYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹', '')
            
            # æ–¹æ³•2: h1ã‚¿ã‚°
            if not title or len(title) < 10:
                h1_tag = soup.find('h1')
                if h1_tag:
                    title = h1_tag.get_text(strip=True)
            
            if not title or len(title) < 10:
                return None
                
            # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
            category = self.categorize_article(title)
            
            # é‡è¦åº¦åˆ¤å®š
            importance = self.assess_importance(title)
            
            return {
                'title': title,
                'url': article_url,
                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                'category': category,
                'importance': importance,
                'summary': f"ã€{category}ã€‘ {title[:80]}{'...' if len(title) > 80 else ''}",
                'keywords': self.extract_keywords(title),
                'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                'scraped_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹è©³ç´°å–å¾—å¤±æ•—: {article_url} - {e}")
            return None
    
    def scrape_mhlw_news(self, max_articles=5):
        """åšç”ŸåŠ´åƒçœã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        articles = []
        
        mhlw_urls = [
            'https://www.mhlw.go.jp/stf/houdou/houdou_list.html',
        ]
        
        for base_url in mhlw_urls:
            print(f"ğŸ›ï¸ åšç”ŸåŠ´åƒçœã‚µã‚¤ãƒˆå–å¾—: {base_url}")
            
            response = self.safe_request(base_url)
            if not response:
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # MHLWã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
            links = soup.find_all('a', href=True)
            
            for link in links:
                if len(articles) >= max_articles:
                    break
                    
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title:
                    continue
                    
                # å®Œå…¨URLã«å¤‰æ›
                if href.startswith('/'):
                    full_url = f"https://www.mhlw.go.jp{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                    
                # ç¤¾ä¼šä¿é™ºé–¢é€£ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self.is_social_insurance_related(title):
                    continue
                    
                # æ—¢å­˜URLã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if any(article['url'] == full_url for article in articles):
                    continue
                    
                # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
                category = self.categorize_article(title)
                
                # é‡è¦åº¦åˆ¤å®š
                importance = self.assess_importance(title)
                
                article = {
                    'title': title,
                    'url': full_url,
                    'source': 'åšç”ŸåŠ´åƒçœ',
                    'category': category,
                    'importance': importance,
                    'summary': f"ã€{category}ã€‘ {title[:80]}{'...' if len(title) > 80 else ''}",
                    'keywords': self.extract_keywords(title),
                    'published_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    'scraped_at': datetime.now().isoformat()
                }
                
                articles.append(article)
                print(f"âœ… MHLWè¨˜äº‹è¿½åŠ : {title[:40]}...")
                
            time.sleep(2)
            
        return articles
    
    def is_social_insurance_related(self, title):
        """ç¤¾ä¼šä¿é™ºé–¢é€£ã®è¨˜äº‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        social_insurance_keywords = [
            'å¹´é‡‘', 'ä¿é™º', 'ç¤¾ä¼šä¿é™º', 'å¥åº·', 'é›‡ç”¨', 'åŠ´ç½', 'ä»‹è­·',
            'åšç”Ÿå¹´é‡‘', 'å›½æ°‘å¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'åŠ´ç½ä¿é™º', 'ä»‹è­·ä¿é™º',
            'ç¤¾ä¼šä¿éšœ', 'åšç”ŸåŠ´åƒçœ', 'ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ã‚¯', 'çµ¦ä»˜', 'ä¿é™ºæ–™'
        ]
        
        return any(keyword in title for keyword in social_insurance_keywords)
    
    def categorize_article(self, title):
        """è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword in title:
                    return category
        return 'ãã®ä»–'
    
    def assess_importance(self, title):
        """è¨˜äº‹ã®é‡è¦åº¦ã‚’è©•ä¾¡"""
        high_priority = ['æ”¹æ­£', 'æ–°åˆ¶åº¦', 'å»ƒæ­¢', 'å¤‰æ›´', 'é‡è¦', 'ç·Šæ€¥', 'é€Ÿå ±', 'æ–½è¡Œ']
        medium_priority = ['å®Ÿæ–½', 'é–‹å§‹', 'å»¶é•·', 'æ‹¡å¤§', 'è¦‹ç›´ã—', 'ç™ºè¡¨']
        
        for keyword in high_priority:
            if keyword in title:
                return 'é«˜'
                
        for keyword in medium_priority:
            if keyword in title:
                return 'ä¸­'
                
        return 'ä½'
    
    def extract_keywords(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        money_pattern = r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?(?:ä¸‡|åƒ|å„„|å…†)?å††)'
        keywords.extend(re.findall(money_pattern, title))
        
        # å¹´æœˆãƒ‘ã‚¿ãƒ¼ãƒ³
        date_pattern = r'(\\d{4}å¹´\\d{1,2}æœˆ|\\d{1,2}æœˆ\\d{1,2}æ—¥|ä»¤å’Œ\\d+å¹´)'
        keywords.extend(re.findall(date_pattern, title))
        
        return keywords[:3]
    
    def run_collection(self):
        """ãƒ¡ã‚¤ãƒ³ã®åé›†å‡¦ç† - GitHubãƒªãƒã‚¸ãƒˆãƒªå½¢å¼"""
        print("ğŸš€ ç¤¾ä¼šä¿é™ºãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†é–‹å§‹ï¼ˆGitHubå½¢å¼ï¼‰")
        
        all_articles = []
        
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰åé›†
        yahoo_terms = ['ç¤¾ä¼šä¿é™º', 'åšç”Ÿå¹´é‡‘', 'å¥åº·ä¿é™º', 'é›‡ç”¨ä¿é™º', 'ä»‹è­·ä¿é™º']
        for term in yahoo_terms:
            articles = self.scrape_yahoo_news_direct(term, max_articles=2)
            all_articles.extend(articles)
            time.sleep(random.uniform(3, 5))
        
        # åšç”ŸåŠ´åƒçœã‹ã‚‰åé›†
        mhlw_articles = self.scrape_mhlw_news(max_articles=5)
        all_articles.extend(mhlw_articles)
        
        # é‡è¤‡é™¤å»
        unique_articles = {}
        for article in all_articles:
            article_key = f"{article['title'][:50]}_{article['url']}"
            if article_key not in unique_articles:
                unique_articles[article_key] = article
        
        final_articles = list(unique_articles.values())
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        categorized = {}
        for article in final_articles:
            category = article['category']
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(article)
        
        # çµ±è¨ˆè¨ˆç®—
        high_importance = sum(1 for article in final_articles if article['importance'] == 'é«˜')
        
        # GitHubå½¢å¼ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        result = {
            'last_updated': datetime.now().isoformat(),
            'total_count': len(final_articles),
            'news': final_articles,  # app.pyãŒæœŸå¾…ã™ã‚‹å½¢å¼
            'categories': categorized
        }
        
        with open('data/processed_news.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        daily_report = {
            'date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'summary': {
                'total_news': len(final_articles),
                'high_importance': high_importance,
                'categories': list(categorized.keys()),
                'top_keywords': self.get_top_keywords(final_articles)
            }
        }
        
        with open('data/daily_report.json', 'w', encoding='utf-8') as f:
            json.dump(daily_report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åé›†å®Œäº†: {len(final_articles)}ä»¶ã®è¨˜äº‹")
        print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(categorized)}")
        print(f"ğŸ”¥ é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹: {high_importance}ä»¶")
        
        for category, articles in categorized.items():
            print(f"   {category}: {len(articles)}ä»¶")
        
        return result
    
    def get_top_keywords(self, articles):
        """è¨˜äº‹ã‹ã‚‰ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keyword_count = {}
        for article in articles:
            for keyword in article.get('keywords', []):
                keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, count in sorted_keywords[:5]]

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = SocialInsuranceNewsScraperFixed()
    scraper.run_collection()

if __name__ == "__main__":
    main()